1.降维方法：
当然，让我们具体化这个例子，我们来看一个假设的数据集，这个数据集由社交网络上用户的图像组成。我们会用一些简化的数字来代表实际的数据点，并说明如何应用PCA、自动编码器和流形学习。

### 数据集假设
假设我们的数据集包含1000张图像，每张图像是一个简化为100个像素的图像。因此，每张图像可以表示为一个100维的点。现在，我们想要理解这些图像的主要变化因素，以及如何将它们有效地表示在更低的维度上。

### 主成分分析（PCA）
1. 我们从将这1000个数据点（图像）输入PCA开始。
2. PCA发现最大的方差方向（主成分），例如第一个主成分可能对应于图像的亮度，第二个主成分可能对应于颜色强度等。
3. 假设我们只保留前10个主成分，我们就可以用一个10维的点来近似原来的100维点，大大减少了数据的复杂性。

### 自动编码器
1. 我们用相同的1000张图像来训练一个自动编码器，自动编码器包含一个编码器和一个解码器。
2. 编码器部分尝试将每个100维的图像压缩到一个更小的维度，比如10维。
3. 解码器部分尝试将这10维的编码重建为原始的100维图像。
4. 通过训练过程，自动编码器学会了如何在低维空间中表示图像，同时仍然能够重建出原始图像的关键特征。

### 流形学习
1. 流形学习方法，如t-SNE或Isomap，将试图找到一个低维表示，它能够维持高维空间中点与点之间的距离。
2. 如果原始的100维空间中图像形成了几个集群（可能对应于不同的场景或对象），流形学习将会创建一个低维映射，这些集群在新空间中仍然分开。
3. 结果可能是一个2维或3维的表示，我们可以在这个低维空间中直观地看到数据点是如何组织的。

### 示例数据点
让我们考虑三张图像（数据点）：
- 图像A：所有100个像素都是中等亮度的灰色，表示一张平均的、无特征的图像。
- 图像B：前50个像素非常亮，后50个像素非常暗，可能代表一张分为明暗两部分的图像。
- 图像C：前25个像素和后25个像素非常亮，中间50个像素非常暗，可能表示一张有两个亮区域的图像。

用PCA处理时，它可能会将图像A作为数据中心，图像B和C的变化可能被识别为主要的变化方向。

自动编码器可能会学习到更复杂的表示，比如它可能会发现图像B和C实际上有着不同的结构特征，尽管它们的亮度分布相似。

流形学习可能会发现这三张图像构成了数据空间的三个不同的“角落”，每个角落代表了图像的不同结构特征。


1. 什么是线性可分问题

- 我为什么会关注这个问题

* 二分类问题判断依据

![image](https://github.com/Chiron-star/chrion.github.io/assets/64126734/a25f02bd-55a1-4579-9aa2-55d784c1a649)
* 多分类判断依据
![image](https://github.com/Chiron-star/chrion.github.io/assets/64126734/2a613714-5781-4476-954f-b23ac62e7747)
* 线性可分问题可选用那些模型
![image](https://github.com/Chiron-star/chrion.github.io/assets/64126734/4b89b7ea-659a-4657-935e-8b413cfd3c1b)
【todo】上面是基于GPT4给出的回复，准确性有待考证，尚需相关论文支撑。
2. 非线性激活函数是必要的吗？

- 背景
在做神经网络泛化性分析时，非线性激活函数的引入的非线性增加了分析的难度，故考虑非线性激活函数的必要性，从理论和实际两方面分析。
-  作用概述

- 具体问题

- 结论

4. 损失函数的等效性

- 背景
在做神经网络泛化性分析时，使用均方差损失更容易分析，而交叉熵则不容易。
- 结论
交叉熵和均方差在某些具体问题上是等效的
- 原因

5. 高斯噪声 and 神经网络泛化性
[高斯噪声_and_神经网络泛化性.pdf](https://github.com/Chiron-star/chrion.github.io/files/14994237/_and_.pdf)
